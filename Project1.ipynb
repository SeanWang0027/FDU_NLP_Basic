{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: 使用Numpy进行文本的四分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤1: 实现数据读取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入pandas读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./train.tsv\",header=0,delimiter=\"\\t\")\n",
    "test_data = pd.read_csv(\"./test.tsv\",header=0,delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   PhraseId    156060 non-null  int64 \n",
      " 1   SentenceId  156060 non-null  int64 \n",
      " 2   Phrase      156060 non-null  object\n",
      " 3   Sentiment   156060 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据标题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算数据比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    0.509945\n",
      "3    0.210989\n",
      "1    0.174760\n",
      "4    0.058990\n",
      "0    0.045316\n",
      "Name: Sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.Sentiment.value_counts()/train_data.Sentiment.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data['Phrase']\n",
    "Y = train_data['Sentiment']\n",
    "test_X = test_data['Phrase']\n",
    "data_train = list(X)\n",
    "label_train = list(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156060"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'series', 'of', 'escapades', 'demonstrating', 'the', 'adage', 'that', 'what', 'is', 'good', 'for', 'the', 'goose', 'is', 'also', 'good', 'for', 'the', 'gander', ',', 'some', 'of', 'which', 'occasionally', 'amuses', 'but', 'none', 'of', 'which', 'amounts', 'to', 'much', 'of', 'a', 'story', '.']\n"
     ]
    }
   ],
   "source": [
    "## 实现文本的分词\n",
    "def get_word(text):\n",
    "    return [word.lower() for word in text.split(' ')]\n",
    "\n",
    "def get_whole_word(data):\n",
    "    return[get_word(text) for text in data]\n",
    "\n",
    "word_list = get_whole_word(data_train)\n",
    "print(word_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现词袋(N-gram)\n",
    "实现二元语法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a series', 'series of', 'of escapades', 'escapades demonstrating', 'demonstrating the', 'the adage', 'adage that', 'that what', 'what is', 'is good', 'good for', 'for the', 'the goose', 'goose is', 'is also', 'also good', 'good for', 'for the', 'the gander', 'gander ,', ', some', 'some of', 'of which', 'which occasionally', 'occasionally amuses', 'amuses but', 'but none', 'none of', 'of which', 'which amounts', 'amounts to', 'to much', 'much of', 'of a', 'a story', 'story .']\n"
     ]
    }
   ],
   "source": [
    "def get_2gram(word_list):\n",
    "    word_bag = []\n",
    "    for words in word_list:\n",
    "        if len(words)==1:\n",
    "            word_bag.append(words)\n",
    "        else:\n",
    "            ngram = [(a+' '+b) for a,b in zip(words[:-1],words[1:])]\n",
    "            word_bag.append(ngram)\n",
    "    return word_bag\n",
    "word_bag = get_2gram(word_list)\n",
    "print(word_bag[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现可迭代的对象\n",
    "`idx_to_char`：一个列表，实现的是从迭代对象中抽取词袋作为一个总列表。\n",
    "\n",
    "`char_to_idx`：一个字典，实现的是词袋和出现的位置的对应（标签号）."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100664\n",
      "83497\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "def get_vocab(word_bag):\n",
    "    counter = collections.Counter([x for sublist in word_bag for x in sublist])\n",
    "    to_char = [item[0] for item in counter.items()]\n",
    "    to_idx = dict([(char,idx)for idx,char in enumerate(to_char)])\n",
    "    return to_char,to_idx\n",
    "idx_to_char,char_to_idx = get_vocab(word_bag)\n",
    "print(len(idx_to_char))\n",
    "print(char_to_idx['jumbled fantasy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现数据集的切割\n",
    "按照`测试集`和`训练集`为1:6的比例进行拆分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129529 129529 26531 26531\n"
     ]
    }
   ],
   "source": [
    "import random # 实现随机打乱原始的dict\n",
    "original_dict = list(zip(data_train,label_train))\n",
    "random.shuffle(original_dict)\n",
    "data_train[:],label_train[:] = zip(*original_dict)\n",
    "len_train = int(len(original_dict) * 0.83)\n",
    "train_phrase = data_train[:len_train]\n",
    "train_label = label_train[:len_train]\n",
    "test_phrase = data_train[len_train:]\n",
    "test_label = label_train[len_train:]\n",
    "print(len(train_phrase),len(train_label),len(test_phrase),len(test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现测试集和训练集的ngram词袋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2gram = get_2gram(get_whole_word(train_phrase))\n",
    "test_2gram =  get_2gram(get_whole_word(test_phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2idx(sentence,idx_list):\n",
    "    try:\n",
    "        return[idx_list[token] for token in sentence]\n",
    "    except (KeyError,TypeError):\n",
    "        # print(sentence)\n",
    "        count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_phrase,train_label,batch_size):\n",
    "    final_data = []\n",
    "    batch_num = len(train_phrase)//batch_size\n",
    "    for i in range(batch_num):\n",
    "        batch_phrase = train_phrase[max(0,i*batch_size):min((i+1)*batch_size,len(train_phrase))]\n",
    "        batch_label = train_label[max(0,i*batch_size):min((i+1)*batch_size,len(train_phrase))]\n",
    "        ngram = []\n",
    "        for sentence in batch_phrase:\n",
    "            ngram.append(sentence2idx(sentence,char_to_idx))\n",
    "        final_data.append((ngram,batch_label))\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_iter = load_data(train_2gram,train_label,batch_size)\n",
    "test_iter = load_data(test_2gram,test_label,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[76575], [85321, 22589, 10453, 30372, 85322, 85323], [11720, 11721, 11722, 11723, 1465, 11724, 11725, 11563, 11726, 11727, 10866, 11728, 11729, 11730, 11731, 11732, 11733, 11734], [1467, 1986, 6410, 77163, 77164, 1497, 35714, 35715, 1467, 4000, 1442, 36873, 25313, 32607, 77165], [21186], [12753, 78711, 13594, 16390, 970, 971, 3446, 1843, 1863, 65514, 43679, 72102, 27332, 78712, 78713], [7228], [30, 79055, 89745, 89746, 89747, 89748], [38813, 4147, 70977, 70978, 70979, 2813, 70980], [14939, 14940, 14941], [27620, 27621, 27622, 27623, 27624, 643, 27625, 27626, 211, 24242, 2194, 27627, 27628, 27629, 21536, 12434, 27630, 27631, 27632, 27633], [259, 86083], [81938], [3691, 59029, 42118], [81349, 81350, 81351, 81352], [81706]]\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_iter:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from tqdm import tqdm \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x_input):\n",
    "    x_input_exp = np.exp(x_input)\n",
    "    partion = np.sum(x_input_exp,axis=1,keepdims=True)\n",
    "    return x_input_exp/partion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小批量初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(x):\n",
    "    batch_size=len(x)\n",
    "    feature_size=len(idx_to_char)\n",
    "    inputs=np.zeros((batch_size,feature_size))\n",
    "    for b,i in enumerate(x):\n",
    "        for idx in i:\n",
    "            inputs[b][idx]=1\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(x,probability,y):\n",
    "    probability[range(probability.shape[0]), y]-=1\n",
    "    dw=x.T.dot(probability)/batch_size #feature_size*n_class\n",
    "    db=np.sum(probability,axis=0)/batch_size #n_class\n",
    "    return dw,db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_iter,W,b):\n",
    "    right=0.0\n",
    "    n=0.0\n",
    "    for x,y in test_iter:\n",
    "        n+=batch_size\n",
    "        x=feature(x)\n",
    "        probability=softmax(np.matmul(x,W)+b)\n",
    "        right+=np.sum(np.argmax(probability,axis=1)==y)\n",
    "    return right/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data,test_data,lr,num_epoch,W,b,batch_size):\n",
    "    for epoch in range(num_epoch):\n",
    "        l_sum,start,n=0.0,time.time(),0\n",
    "        train_iter=iter(train_data)\n",
    "        test_iter=iter(test_data)\n",
    "        for x,y in tqdm(train_iter):\n",
    "            x=feature(x) #[batch_size,feature]\n",
    "            probability=softmax(np.matmul(x,W)+b) #[batch_size,n_class]\n",
    "            loss= np.sum(-np.log(probability[range(probability.shape[0]), y]))\n",
    "            grad_w,grad_b=backward(x,probability,y)\n",
    "            #print(x.shape,probability.shape,loss.shape,grad_w.shape,grad_b.shape)\n",
    "            #print(grad_w,grad_b)\n",
    "            W=W-lr*grad_w\n",
    "            b=b-lr*grad_b\n",
    "            l_sum+=loss\n",
    "            n+=1\n",
    "            #print(loss)\n",
    "        print(\"epoch %d ,loss %.3f ,test_acc %.2f,time %.2f\"%(epoch+1,l_sum/n,evaluate(test_iter,W,b),time.time()-start))\n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[76575], [85321, 22589, 10453, 30372, 85322, 85323], [11720, 11721, 11722, 11723, 1465, 11724, 11725, 11563, 11726, 11727, 10866, 11728, 11729, 11730, 11731, 11732, 11733, 11734], [1467, 1986, 6410, 77163, 77164, 1497, 35714, 35715, 1467, 4000, 1442, 36873, 25313, 32607, 77165], [21186], [12753, 78711, 13594, 16390, 970, 971, 3446, 1843, 1863, 65514, 43679, 72102, 27332, 78712, 78713], [7228], [30, 79055, 89745, 89746, 89747, 89748], [38813, 4147, 70977, 70978, 70979, 2813, 70980], [14939, 14940, 14941], [27620, 27621, 27622, 27623, 27624, 643, 27625, 27626, 211, 24242, 2194, 27627, 27628, 27629, 21536, 12434, 27630, 27631, 27632, 27633], [259, 86083], [81938], [3691, 59029, 42118], [81349, 81350, 81351, 81352], [81706], [70052, 4713, 15543, 13348, 70053, 70054], [54065, 11166, 43292], [713, 18276], [86273, 86274], [316, 20494, 27212], [85448], [47854, 2965, 58441, 5646, 2251, 58442, 58443, 58444, 58445, 58446, 58447, 58448, 511, 9312, 58449, 58450, 58451, 58452, 58453, 58454], [16735, 97758, 97759, 97760, 97761, 97762, 97763, 97764, 97765, 97766, 97767, 7274], [245, 14629, 14630, 14631, 14632, 14633, 14634, 2289, 14635, 14636, 14637, 3266], [62084], [83167], [10168, 39438], [19020], [3921, 38464, 77110, 77111, 77112, 77113, 77114, 77115, 77116, 77117, 77118, 77119, 26501, 77120, 77121, 77122, 77123, 77124, 17561, 77125, 77126, 5594, 526, 2270, 971, 3446, 2813, 77127, 77128, 77129, 77130], [95753], [93022, 93023, 33200, 18757, 5976, 71984, 93024, 35976], [3446, 3447, 13089, 74415], [98332, 27092, 98333, 98334, 98335, 98336, 98337, 56268, 16015, 35018, 98338, 5545, 2309, 98339, 69794, 98340, 98341], [6317, 20628, 83848, 83849, 83850, 44311, 75830], [31792, 50560, 50561], [12925, 12926], [39093, 39094, 511, 8131], [67794], [563, 582, 94358, 94359], [62628], [7897, 51050, 33173, 79298, 79299, 1334, 1335, 47455, 79300], [38981, 85840, 1584, 7955, 85841, 85842, 8843, 85843, 85844, 58351], [3735, 312, 41523, 62351], [7949, 82441, 82442, 82443, 10082, 3631, 3632, 705, 4290, 366, 82444, 82445, 24335, 311, 82446, 82447, 39494, 82448, 82449], [70275, 70276, 51207], [53062, 53063], [1683, 1684, 1685, 1686, 1687], [7034], [95085, 95086, 95087, 95088, 788], [6131], [46789, 6903, 51940, 51941, 51942], [23584, 47209, 67989, 87223, 2819, 736], [7356, 7357], [60398, 60399, 47215, 60400], [29305, 11, 543, 544, 5176, 14529, 14530, 12690], [4737], [24816], [97567, 97568], [526, 21117, 21118, 21119, 21120, 21121, 21122, 5594, 5976, 21123, 21124, 5124, 21125], [73429, 73430], [11893, 8035, 40634], [2140, 74597], [71612, 71613, 71614, 71615, 71616]], [2, 3, 1, 1, 2, 1, 1, 3, 1, 1, 3, 2, 2, 1, 2, 3, 2, 1, 3, 2, 3, 1, 1, 0, 1, 2, 1, 1, 2, 4, 3, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 1, 4, 2, 2, 1, 2, 3, 2, 2, 3, 2, 2, 2, 1, 2, 1, 2, 3, 1, 2, 3, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "train_iter=load_data(train_2gram,train_label,batch_size)\n",
    "test_iter=load_data(test_2gram,test_label,batch_size)\n",
    "feature_size=len(idx_to_char)\n",
    "n_class=5\n",
    "W=np.random.normal(0,0.01,(feature_size,n_class))\n",
    "b=np.zeros(n_class)\n",
    "lr,num_epoch=0.01,10\n",
    "print(train_iter[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023it [00:38, 52.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 ,loss 84.909 ,test_acc 0.51,time 42.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023it [00:38, 52.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 ,loss 81.760 ,test_acc 0.51,time 42.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023it [00:38, 52.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 ,loss 81.317 ,test_acc 0.51,time 42.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023it [00:40, 50.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 ,loss 80.964 ,test_acc 0.51,time 44.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023it [00:38, 52.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 ,loss 80.649 ,test_acc 0.52,time 42.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023it [00:38, 52.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 ,loss 80.362 ,test_acc 0.52,time 42.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023it [00:39, 51.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 ,loss 80.095 ,test_acc 0.52,time 43.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023it [00:38, 52.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 ,loss 79.846 ,test_acc 0.52,time 42.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023it [00:38, 52.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 ,loss 79.610 ,test_acc 0.52,time 42.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023it [00:39, 51.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 ,loss 79.386 ,test_acc 0.52,time 42.89\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0.00481624, -0.01876387,  0.00425946, -0.0020165 ,  0.00392172],\n",
       "        [ 0.01699159,  0.00447008, -0.00694894,  0.0004948 , -0.00068289],\n",
       "        [ 0.01479213, -0.02134954, -0.02270377, -0.01470759,  0.00015227],\n",
       "        ...,\n",
       "        [-0.01166653,  0.0151503 , -0.01169591, -0.02102606,  0.00221335],\n",
       "        [-0.0101952 ,  0.00664214,  0.00657163, -0.00285486, -0.00502911],\n",
       "        [ 0.01231343,  0.01513464, -0.01066177, -0.00928344, -0.01183595]]),\n",
       " array([-1.13175303,  0.19544835,  1.42866685,  0.39233987, -0.88470205]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(train_iter,test_iter,lr,num_epoch,W,b,batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
