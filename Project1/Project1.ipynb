{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: 使用Numpy进行文本的四分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤1: 实现数据读取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入pandas读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./train.tsv\",header=0,delimiter=\"\\t\")\n",
    "test_data = pd.read_csv(\"./test.tsv\",header=0,delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   PhraseId    156060 non-null  int64 \n",
      " 1   SentenceId  156060 non-null  int64 \n",
      " 2   Phrase      156060 non-null  object\n",
      " 3   Sentiment   156060 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据标题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算数据比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    0.509945\n",
      "3    0.210989\n",
      "1    0.174760\n",
      "4    0.058990\n",
      "0    0.045316\n",
      "Name: Sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.Sentiment.value_counts()/train_data.Sentiment.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data['Phrase']\n",
    "Y = train_data['Sentiment']\n",
    "test_X = test_data['Phrase']\n",
    "data_train = list(X)\n",
    "label_train = list(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156060"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'series', 'of', 'escapades', 'demonstrating', 'the', 'adage', 'that', 'what', 'is', 'good', 'for', 'the', 'goose', 'is', 'also', 'good', 'for', 'the', 'gander', ',', 'some', 'of', 'which', 'occasionally', 'amuses', 'but', 'none', 'of', 'which', 'amounts', 'to', 'much', 'of', 'a', 'story', '.']\n"
     ]
    }
   ],
   "source": [
    "## 实现文本的分词\n",
    "def get_word(text):\n",
    "    return [word.lower() for word in text.split(' ')]\n",
    "\n",
    "def get_whole_word(data):\n",
    "    return[get_word(text) for text in data]\n",
    "\n",
    "word_list = get_whole_word(data_train)\n",
    "print(word_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现词袋(N-gram)\n",
    "实现二元语法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a series', 'series of', 'of escapades', 'escapades demonstrating', 'demonstrating the', 'the adage', 'adage that', 'that what', 'what is', 'is good', 'good for', 'for the', 'the goose', 'goose is', 'is also', 'also good', 'good for', 'for the', 'the gander', 'gander ,', ', some', 'some of', 'of which', 'which occasionally', 'occasionally amuses', 'amuses but', 'but none', 'none of', 'of which', 'which amounts', 'amounts to', 'to much', 'much of', 'of a', 'a story', 'story .']\n"
     ]
    }
   ],
   "source": [
    "def get_2gram(word_list):\n",
    "    word_bag = []\n",
    "    for words in word_list:\n",
    "        if len(words)==1:\n",
    "            word_bag.append(words)\n",
    "        else:\n",
    "            ngram = [(a+' '+b) for a,b in zip(words[:-1],words[1:])]\n",
    "            word_bag.append(ngram)\n",
    "    return word_bag\n",
    "word_bag = get_2gram(word_list)\n",
    "print(word_bag[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现可迭代的对象\n",
    "`idx_to_char`：一个列表，实现的是从迭代对象中抽取词袋作为一个总列表。\n",
    "\n",
    "`char_to_idx`：一个字典，实现的是词袋和出现的位置的对应（标签号）."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100664\n",
      "83497\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "def get_vocab(word_bag):\n",
    "    counter = collections.Counter([x for sublist in word_bag for x in sublist])\n",
    "    to_char = [item[0] for item in counter.items()]\n",
    "    to_idx = dict([(char,idx)for idx,char in enumerate(to_char)])\n",
    "    return to_char,to_idx\n",
    "idx_to_char,char_to_idx = get_vocab(word_bag)\n",
    "print(len(idx_to_char))\n",
    "print(char_to_idx['jumbled fantasy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现数据集的切割\n",
    "按照`测试集`和`训练集`为1:6的比例进行拆分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129529 129529 26531 26531\n"
     ]
    }
   ],
   "source": [
    "import random # 实现随机打乱原始的dict\n",
    "original_dict = list(zip(data_train,label_train))\n",
    "random.shuffle(original_dict)\n",
    "data_train[:],label_train[:] = zip(*original_dict)\n",
    "len_train = int(len(original_dict) * 0.83)\n",
    "train_phrase = data_train[:len_train]\n",
    "train_label = label_train[:len_train]\n",
    "test_phrase = data_train[len_train:]\n",
    "test_label = label_train[len_train:]\n",
    "print(len(train_phrase),len(train_label),len(test_phrase),len(test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现测试集和训练集的ngram词袋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2gram = get_2gram(get_whole_word(train_phrase))\n",
    "test_2gram =  get_2gram(get_whole_word(test_phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2idx(sentence,idx_list):\n",
    "    try:\n",
    "        return[idx_list[token] for token in sentence]\n",
    "    except (KeyError,TypeError):\n",
    "        # print(sentence)\n",
    "        count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_phrase,train_label,batch_size):\n",
    "    final_data = []\n",
    "    batch_num = len(train_phrase)//batch_size\n",
    "    for i in range(batch_num):\n",
    "        batch_phrase = train_phrase[max(0,i*batch_size):min((i+1)*batch_size,len(train_phrase))]\n",
    "        batch_label = train_label[max(0,i*batch_size):min((i+1)*batch_size,len(train_phrase))]\n",
    "        ngram = []\n",
    "        for sentence in batch_phrase:\n",
    "            ngram.append(sentence2idx(sentence,char_to_idx))\n",
    "        final_data.append((ngram,batch_label))\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_iter = load_data(train_2gram,train_label,batch_size)\n",
    "test_iter = load_data(test_2gram,test_label,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29645], [3091], [87148, 87149], [9056], [734, 735, 2746, 2747, 12707], [64703, 15890], [173, 3008, 27122], [5211, 4960, 5212, 2289, 5213, 5214, 3445, 3446, 5215, 5216, 5217, 5218, 5219, 5220, 5221, 5222, 5223], [21373, 21374, 7734, 21375, 21376, 21377, 21378, 21379, 1467, 1130, 9555, 21380, 9973, 21381, 21382], [61806, 17866, 61807, 61808], [1986, 1987, 13119, 13120, 6001, 452, 2227, 78649, 78650, 78651, 78652, 47388, 78653, 78654, 78655, 1909, 31708, 19042], [8068, 788, 2532], [64100, 35056, 64101, 56602], [97416, 70021, 97417, 2130, 57854, 59567, 1736, 37659, 97418], [83786], [42273, 94962, 62382, 94963, 4664, 41693, 4405, 18848, 27393, 25102, 94964, 35938, 94965, 94966, 49829, 19, 511, 2960, 77427, 17799, 94967, 94968, 46841, 7540, 25467, 94969, 94970, 94971, 94972, 14627]]\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_iter:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from tqdm import tqdm \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x_input):\n",
    "    x_input_exp = np.exp(x_input)\n",
    "    partion = np.sum(x_input_exp,axis=1,keepdims=True)\n",
    "    return x_input_exp/partion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小批量初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(x):\n",
    "    batch_size=len(x)\n",
    "    feature_size=len(idx_to_char)\n",
    "    inputs=np.zeros((batch_size,feature_size))\n",
    "    for b,i in enumerate(x):\n",
    "        for idx in i:\n",
    "            inputs[b][idx]=1\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(x,probability,y):\n",
    "    probability[range(probability.shape[0]), y]-=1\n",
    "    dw=x.T.dot(probability)/batch_size #feature_size*n_class\n",
    "    db=np.sum(probability,axis=0)/batch_size #n_class\n",
    "    return dw,db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_iter,W,b):\n",
    "    right=0.0\n",
    "    n=0.0\n",
    "    for x,y in test_iter:\n",
    "        n+=batch_size\n",
    "        x=feature(x)\n",
    "        probability=softmax(np.matmul(x,W)+b)\n",
    "        right+=np.sum(np.argmax(probability,axis=1)==y)\n",
    "    return right/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data,test_data,lr,num_epoch,W,b,batch_size):\n",
    "    for epoch in range(num_epoch):\n",
    "        l_sum,start,n=0.0,time.time(),0\n",
    "        train_iter=iter(train_data)\n",
    "        test_iter=iter(test_data)\n",
    "        for x,y in tqdm(train_iter):\n",
    "            x=feature(x) #[batch_size,feature]\n",
    "            probability=softmax(np.matmul(x,W)+b) #[batch_size,n_class]\n",
    "            loss= np.sum(-np.log(probability[range(probability.shape[0]), y]))\n",
    "            grad_w,grad_b=backward(x,probability,y)\n",
    "            #print(x.shape,probability.shape,loss.shape,grad_w.shape,grad_b.shape)\n",
    "            #print(grad_w,grad_b)\n",
    "            W=W-lr*grad_w\n",
    "            b=b-lr*grad_b\n",
    "            l_sum+=loss\n",
    "            n+=1\n",
    "            #print(loss)\n",
    "        print(\"epoch %d ,loss %.3f ,test_acc %.2f,time %.2f\"%(epoch+1,l_sum/n,evaluate(test_iter,W,b),time.time()-start))\n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[29645], [3091], [87148, 87149], [9056], [734, 735, 2746, 2747, 12707], [64703, 15890], [173, 3008, 27122], [5211, 4960, 5212, 2289, 5213, 5214, 3445, 3446, 5215, 5216, 5217, 5218, 5219, 5220, 5221, 5222, 5223], [21373, 21374, 7734, 21375, 21376, 21377, 21378, 21379, 1467, 1130, 9555, 21380, 9973, 21381, 21382], [61806, 17866, 61807, 61808], [1986, 1987, 13119, 13120, 6001, 452, 2227, 78649, 78650, 78651, 78652, 47388, 78653, 78654, 78655, 1909, 31708, 19042], [8068, 788, 2532], [64100, 35056, 64101, 56602], [97416, 70021, 97417, 2130, 57854, 59567, 1736, 37659, 97418], [83786], [42273, 94962, 62382, 94963, 4664, 41693, 4405, 18848, 27393, 25102, 94964, 35938, 94965, 94966, 49829, 19, 511, 2960, 77427, 17799, 94967, 94968, 46841, 7540, 25467, 94969, 94970, 94971, 94972, 14627], [23477, 73595, 73596], [31469, 31470], [95364, 42490, 95365, 95366, 95367, 154, 643, 95368, 80485, 5534], [66223, 46938, 43801, 154, 35204], [96040, 96041], [526, 2440, 37093, 7154], [27896, 27897, 2358, 27898, 27899, 27900, 27901, 3709, 27902, 18930, 27903, 27904, 27905, 27906, 6129], [7600], [1585, 6918, 6919, 6920, 6921, 4231, 6922, 6923, 6924], [15183], [73030, 73031], [13510, 13511], [30863, 81615, 38376, 38377], [16203, 16204], [13060, 61138, 19820, 363, 61139, 61140, 12473, 12474, 33946, 61141, 61142, 61143, 60447, 25181, 61144, 61145, 61146], [35], [55805], [97768, 97769], [43985, 63476, 2529, 63477, 63478, 38632, 26000, 23328, 13994, 19711, 2743, 58579, 10286, 63479, 63480, 63481, 63482, 63483, 7676, 7677], [74239], [13492, 13493, 5907, 23551, 40882, 33173, 311, 7209, 12880, 40883, 25153, 40884, 40885, 40886, 788, 243, 526, 18140, 40887, 534, 810, 40888, 40889, 40890, 40891], [921, 563, 16889, 89, 75007, 30581, 1438, 4710, 9503, 75008, 75009, 75010, 15124, 75011, 75012, 75013], [27151, 42235, 11376, 1519, 45981, 45982, 45983, 788, 7761, 890, 45984, 45985, 45986, 45987, 45988], [33516], [80619, 80620, 80621, 24511, 80622, 34621, 605, 7058, 80623, 80624, 80625, 80626, 80627, 80628, 80629, 80630], [83091, 83092], [2014, 2440, 43890, 43891, 43892, 43893, 43894], [7106, 14300, 14301, 14302, 14303, 14304, 14305, 14306], [68537], [5976, 7021, 69150, 15778, 69151, 69152, 69153], [15753, 15754, 15755, 15756, 15757, 15758, 15759], [54567], [266, 6689, 28227, 28228, 28229, 7041, 29, 511, 28230, 28231, 28232, 28233, 6007, 28234, 28235, 9616, 28236, 28237, 10082, 28238, 28239, 90, 643, 28240, 28241, 1605, 2826, 28242, 14401, 7170, 28243, 28244, 28245, 97], [27688, 7540, 98857], [2849, 79966, 79967, 23010, 71636, 79968, 79969, 79970, 79971, 13198, 23386, 13554, 79972], [37143, 37144, 37145, 15148, 37146, 37147, 37148, 37149, 9911, 14390, 33048, 3809, 37150, 37151, 19823, 511, 5446, 14236], [71140], [13425, 13426, 97098, 97099, 97100], [2166, 99356, 7173, 40684, 99357], [46434], [14395], [4651, 259, 5270, 1793, 4772, 5271], [28371, 28372, 19659, 28373, 28374, 28375, 28376, 25522, 28377, 28378, 27513, 16280, 10820, 28379, 28380], [5884], [35160], [610, 12979, 80117, 80118, 80119, 80120, 80121, 80122, 80123, 80124, 80125, 3709, 307, 80126, 28995, 316, 80127, 80128, 80129, 80130, 80131], [13282], [23680, 23681, 8734, 23682, 23683]], [2, 2, 1, 2, 2, 3, 2, 1, 3, 2, 1, 1, 1, 2, 2, 3, 3, 2, 3, 3, 2, 2, 1, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 1, 3, 3, 2, 3, 2, 0, 2, 1, 3, 3, 3, 2, 1, 4, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "train_iter=load_data(train_2gram,train_label,batch_size)\n",
    "test_iter=load_data(test_2gram,test_label,batch_size)\n",
    "feature_size=len(idx_to_char)\n",
    "n_class=5\n",
    "W=np.random.normal(0,0.01,(feature_size,n_class))\n",
    "b=np.zeros(n_class)\n",
    "lr,num_epoch=2,10\n",
    "print(train_iter[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023it [00:36, 56.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 ,loss 74.248 ,test_acc 0.58,time 39.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023it [00:39, 51.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 ,loss 64.036 ,test_acc 0.59,time 43.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023it [00:39, 50.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 ,loss 59.030 ,test_acc 0.61,time 43.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023it [00:39, 51.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 ,loss 55.632 ,test_acc 0.61,time 43.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023it [00:39, 51.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 ,loss 53.043 ,test_acc 0.62,time 43.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023it [00:39, 51.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 ,loss 50.942 ,test_acc 0.62,time 43.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023it [00:39, 51.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 ,loss 49.170 ,test_acc 0.62,time 43.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023it [00:39, 51.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 ,loss 47.636 ,test_acc 0.62,time 43.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023it [00:39, 51.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 ,loss 46.281 ,test_acc 0.63,time 43.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023it [00:39, 50.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 ,loss 45.066 ,test_acc 0.63,time 43.62\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.08647244, -0.51775476,  0.61073518, -0.07400414,  0.06984512],\n",
       "        [ 0.39195092,  0.00178828, -0.11357876, -0.25443868, -0.02323411],\n",
       "        [-0.00272594,  0.22928693, -0.1477594 , -0.06485332, -0.00716567],\n",
       "        ...,\n",
       "        [-0.0074    , -0.0362683 ,  0.09948904, -0.03943964, -0.01867478],\n",
       "        [-0.01533404, -0.03966703,  0.09706932, -0.04298233,  0.00543835],\n",
       "        [-0.02363159, -0.03340608,  0.09609087, -0.03272271, -0.01144012]]),\n",
       " array([-1.57507385,  0.11534904,  2.08727295,  0.52331056, -1.15085869]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(train_iter,test_iter,lr,num_epoch,W,b,batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
